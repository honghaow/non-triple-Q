{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Vjyx07xA7w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUMsVqYFubJw"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "grids = 15\n",
        "data = np.zeros((grids,grids))\n",
        "data[:,0] = 1\n",
        "data[:,grids-1] = 1\n",
        "data[0,:] = 1\n",
        "data[grids-1,:] = 1\n",
        "\n",
        "ratio = 0.40\n",
        "for i in range(2,grids-2,1):\n",
        "  for j in range(1,grids-1,1):\n",
        "    if np.random.rand() <= ratio:\n",
        "      data[i,j] = 2\n",
        "data[8, 9] =  2\n",
        "data[3,13] =  2\n",
        "data[9,10] =  2\n",
        "data[9,11] =  2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "JmkZTvZvw-_4",
        "outputId": "030ab781-0d19-4d1a-f70a-2e6775081fc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "startx,starty = grids-2,grids-2\n",
        "data[startx,starty] = 3 #starting point\n",
        "goal_y = int(np.random.rand() * (grids-3)) \n",
        "data[1,goal_y+1] = 4 #destination\n",
        "'''\n",
        "#0:normal 1:wall 2:obstacle 3:starting point 4:destination\n",
        "'''\n",
        "cmap = colors.ListedColormap(['black','silver','orange','red','green'])\n",
        "fig,ax = plt.subplots(1,figsize=(8,8))\n",
        "plt.pcolor(data[::-1],cmap=cmap, linewidths=1)\n",
        "plt.xticks(np.arange(0.5,grids+0.5,step=1))\n",
        "plt.yticks(np.arange(0.5,grids+0.5,step=1))\n",
        "xlabels = ['14','13','12','11','10','9','8','7','6','5','4','3','2','1','0']\n",
        "ylabels = ['14','13','12','11','10','9','8','7','6','5','4','3','2','1','0']\n",
        "#ylabels = ['4','3','2','1','0']\n",
        "ax.set_yticklabels(ylabels)\n",
        "ax.set_xticklabels(xlabels[::-1])\n",
        "ax.xaxis.tick_top()\n",
        "#plt.scatter(1, 24, s=500,c='b', marker=(5, 1))\n",
        "#plt.scatter(0.5, 4.5, s=500,c='r', marker=(5, 1))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzaOjyU3ySsJ"
      },
      "outputs": [],
      "source": [
        "actions_space = ['U','D','L','R']\n",
        "from numpy import linalg as LA\n",
        "        \n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "class  GridWorld():\n",
        "    def __init__(self,K,reward_b = 0.1,cost_b=0.1):\n",
        "        self.current_location = [startx,starty]\n",
        "        self.max_distance = LA.norm(np.array([18,1]) - np.array([1,goal_y+1]))\n",
        "        self.delta = 0.05 \n",
        "        self.reward_change = np.random.randint(2, size= (grids,grids)) * 2 - 1\n",
        "        self.variation_r = np.zeros((grids,grids))\n",
        "        self.variation_c = 0\n",
        "        self.K = K\n",
        "        self.reward_b = reward_b\n",
        "        self.cost_b = cost_b\n",
        "        self.action_space = ['U','D','L','R']\n",
        "        \n",
        "    def variation(self):\n",
        "        self.variation_r += self.reward_change * self.reward_b / self.K\n",
        "        self.variation_c += self.cost_b / self.K\n",
        "        self.delta += 0.1 / self.K\n",
        "\n",
        "    def reset_location(self):\n",
        "        self.current_location = [startx,starty]\n",
        "        \n",
        "    def get_reward(self, location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        norm_r = LA.norm(np.array(location) - np.array([1,goal_y+1]))\n",
        "        if data[location[0],location[1]] == 4:\n",
        "            r = 1\n",
        "        else:\n",
        "            r = max( ((self.max_distance - norm_r) / self.max_distance) * 0.1 + self.variation_r[location[0],location[1]],0)\n",
        "        return r\n",
        "\n",
        "    def check_border(self,location):\n",
        "        if location[0] <= 0 or location[0] >= grids-1 :\n",
        "            return 1\n",
        "        elif location[1] <= 0 or location[1] >= grids-1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def make_step(self, action):\n",
        "        # Store previous location\n",
        "        last_location = self.current_location\n",
        "        if np.random.rand() <= self.delta:\n",
        "            action = self.action_space[np.random.choice(4,1)[0]]\n",
        "        # UP\n",
        "        if action == 'U':\n",
        "            border_ = self.check_border([last_location[0]-1, last_location[1]])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location) \n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0] - 1, last_location[1]]) \n",
        "                self.current_location = [last_location[0] - 1, last_location[1]]             \n",
        "        # DOWN\n",
        "        elif action == 'D':\n",
        "            border_ = self.check_border([last_location[0] + 1, last_location[1]])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0] + 1, last_location[1]]) \n",
        "                self.current_location = [last_location[0] + 1, last_location[1]]\n",
        "\n",
        "        elif action == 'L':\n",
        "            border_ = self.check_border([last_location[0], last_location[1] - 1])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0], last_location[1] - 1]) \n",
        "                self.current_location = [last_location[0], last_location[1] - 1]\n",
        "                \n",
        "        elif action == 'R':\n",
        "            border_ = self.check_border([last_location[0], last_location[1] + 1])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0], last_location[1] + 1]) \n",
        "                self.current_location = [last_location[0], last_location[1] + 1]\n",
        "        d = 0\n",
        "        cost = 0.9 + self.variation_c if data[self.current_location[0],self.current_location[1]] == 2 else 0 \n",
        "                         \n",
        "        return reward, cost, d\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng_9Lyw31AA_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Q_Agent_cmdp():\n",
        "    # Intialization\n",
        "    def __init__(self,env,use_lr=True,use_ucb=True,lr=0.1,use_greddy=True,use_epsilon=False,show_z=True,decay_step=500,total_steps=40000):\n",
        "        self.env = env\n",
        "        self.actions_space = ['U','D','L','R']\n",
        "        self.H = 20\n",
        "        self.S = (grids-2)*(grids-2)\n",
        "        self.A = 4\n",
        "        self.K = total_steps\n",
        "        self.decay_step = decay_step\n",
        "        if use_greddy:\n",
        "          self.epsilon = 1\n",
        "        else:\n",
        "          self.epsilon = 0\n",
        "        self.variation = 0.6 * self.H\n",
        "        self.episode_reward = []\n",
        "        self.Z = 0\n",
        "        self.qtable = np.zeros((self.H,grids,grids,4)) + 20\n",
        "        self.ctable = np.zeros((self.H,grids,grids,4)) + 20\n",
        "        self.vtable = np.zeros((self.H,grids,grids)) + 20\n",
        "        self.wtable = np.zeros((self.H,grids,grids)) + 20\n",
        "\n",
        "        self.decay_step = decay_step\n",
        "        self.vtable[self.H-1,:,:] = 0\n",
        "        self.wtable[self.H-1,:,:] = 0\n",
        "        self.numbers = np.zeros((self.H,grids,grids,4))\n",
        "        self.frame_length =  3 * int(self.K ** 0.6 / self.variation ** (2./3.))\n",
        "        self.rho = 15\n",
        "        self.iota =  math.log(math.sqrt(2*self.A*self.H)*self.K)\n",
        "        self.epsilon_z =  1e-4 * 8 * math.sqrt(self.A*(self.H ** 6) * self.iota) * self.variation ** (1./3.) / (self.K ** 0.2)\n",
        "        self.epsilon = 1\n",
        "        self.eta = 0.1 * self.K ** 0.2 * self.variation ** (1./3.)\n",
        "        print(\"eta is {}\".format(self.eta))\n",
        "        print(f\"epsilon is {self.epsilon_z}  variation is {self.variation}\")\n",
        "        print(\"frame size is {}\".format(self.frame_length))\n",
        "        self.qhi = self.K ** 0.2\n",
        "        self.episode_reward = [] \n",
        "        self.episode_cost = []\n",
        "        self.lr = lr\n",
        "        self.use_lr = use_lr\n",
        "        self.use_ucb = use_ucb\n",
        "        self.show_z=show_z\n",
        "        self.use_epsilon = use_epsilon\n",
        "        \n",
        "    def choose_action(self, h, state):\n",
        "        if self.use_epsilon:\n",
        "            if np.random.rand() < self.epsilon:\n",
        "                a_ = np.random.choice(4,1)\n",
        "                action = self.actions_space[a_[0]]\n",
        "                return action\n",
        "        actions = self.qtable[h,state[0], state[1],:] + (self.Z / self.eta) * self.ctable[h,state[0], state[1],:]\n",
        "        action_index = np.random.choice(np.where(actions == actions.max())[0])\n",
        "        return self.actions_space[int(action_index)]\n",
        "\n",
        "    def use_action(self, h, state):\n",
        "        actions = self.qtable[h,state[0], state[1],:] + (self.Z / self.eta) * self.ctable[h,state[0], state[1],:]\n",
        "        action_index = np.random.choice(np.where(actions == actions.max())[0])\n",
        "        return self.actions_space[int(action_index)]\n",
        "\n",
        "    \n",
        "    def learn(self, states, actions, rewards, costs, dones):\n",
        "        for i in range(len(states)-1):\n",
        "            a_index = self.actions_space.index(actions[i])\n",
        "            self.vtable[i,states[i][0], states[i][1]] = self.qtable[i,states[i][0], states[i][1],a_index]\n",
        "            self.wtable[i,states[i][0], states[i][1]] = self.ctable[i,states[i][0], states[i][1],a_index]\n",
        "            \n",
        "        for i in range(len(states)-1):\n",
        "            a_index = self.actions_space.index(actions[i])\n",
        "            self.numbers[i, states[i][0],states[i][1],a_index] += 1\n",
        "            t = self.numbers[i,states[i][0],states[i][1],a_index]\n",
        "            alpha = 1 * (self.qhi + 1.) / (self.qhi + t) if self.use_lr else self.lr\n",
        "            b =  0.2 * np.sqrt( np.log(self.K ) / self.numbers[i,states[i][0],states[i][1],a_index]) if self.use_ucb else 0 \n",
        "            bb = 2 * self.H * self.variation ** (1./3.) / self.K ** (0.4)\n",
        "            if dones[i]:\n",
        "                self.qtable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.qtable[i,states[i][0], states[i][1], a_index] + alpha * (rewards[i] +  0.1 * b + 0.1 * bb)\n",
        "                self.ctable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.ctable[i,states[i][0], states[i][1], a_index] + alpha * (costs[i]+ 0.1 * b + 0.01 * bb)\n",
        "            else:\n",
        "                self.qtable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.qtable[i,states[i][0], states[i][1], a_index] \\\n",
        "                                                                    + alpha * (rewards[i] + self.vtable[i+1,states[i+1][0],states[i+1][1]] + 0.1 * b + 0.1 * bb)\n",
        "                self.ctable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.ctable[i,states[i][0], states[i][1], a_index] \\\n",
        "                                                                    + alpha * (costs[i] + self.wtable[i+1,states[i+1][0],states[i+1][1]] +  0.1 * b + 0.01 * bb)\n",
        "        \n",
        "    def update_z(self,cbar):\n",
        "        self.Z = max(self.Z + self.rho + self.epsilon_z - cbar / self.frame_length , 0)\n",
        "        print(self.rho,self.epsilon_z,cbar / self.frame_length)\n",
        "        \n",
        "    def play(self):\n",
        "        cbar = 0\n",
        "        test_score = 0\n",
        "        total_steps = 0\n",
        "        collision = 0\n",
        "        avg_cost_total = []\n",
        "        test_rewards = []\n",
        "        test_costs = []\n",
        "        \n",
        "        for trial in range(self.K): # Run trials\n",
        "            if trial > 1.75e6:\n",
        "                break\n",
        "            cumulative_reward = 0 # Initialise values of each game\n",
        "            cumulative_cost = 0\n",
        "            self.env.reset_location()\n",
        "            actions_epi = []\n",
        "            locations = []\n",
        "            wall_time = 0\n",
        "            \n",
        "            if trial % self.frame_length == 0 and trial != 0:\n",
        "                self.qtable = np.zeros((self.H,grids,grids,4)) + 20\n",
        "                self.ctable = np.zeros((self.H,grids,grids,4)) + 20\n",
        "                self.epsilon = 1\n",
        "                self.update_z(cbar)\n",
        "                if self.show_z:\n",
        "                  print(\"z at frmae {} is {}\".format((trial / self.frame_length),self.Z))\n",
        "                  print(\"cbar at frame {} is {}\".format((trial / self.frame_length),(cbar / self.frame_length)))\n",
        "                self.numbers = np.zeros((self.H,grids,grids,4))\n",
        "                cbar = 0.\n",
        "            \n",
        "             \n",
        "            states = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            costs = []\n",
        "            dones = []\n",
        "            states.append(self.env.current_location)\n",
        "          \n",
        "            for h in range(self.H):\n",
        "                old_state = self.env.current_location\n",
        "                action = self.choose_action(h,old_state)\n",
        "                actions.append(action)\n",
        "                \n",
        "                reward, cost, done = self.env.make_step(action)\n",
        "                cumulative_cost += cost\n",
        "                cost =  1 - cost\n",
        "                new_state = self.env.current_location \n",
        "                states.append(new_state)\n",
        "                rewards.append(reward)\n",
        "                costs.append(cost)\n",
        "                \n",
        "                if (cost) == 0:\n",
        "                    wall_time += 1\n",
        "                    \n",
        "                total_steps += 1\n",
        "                actions_epi.append(action)          \n",
        "                cumulative_reward += reward\n",
        "              \n",
        "                done = 1 if h == self.H -1 else done\n",
        "                dones.append(done)\n",
        "                  \n",
        "            self.learn(states, actions, rewards, costs, dones)\n",
        "            if trial % self.decay_step == 0:\n",
        "                self.epsilon = self.epsilon * 0.95\n",
        "      \n",
        "            a1 = self.actions_space.index(actions[0])\n",
        "            cbar += self.ctable[0,states[0][0],states[0][1],a1]\n",
        "            \n",
        "            self.episode_reward.append(cumulative_reward)\n",
        "            self.episode_cost.append(cumulative_cost) \n",
        "\n",
        "            if trial % 10000 == 0:\n",
        "                print (f\"mean reward: {np.mean(self.episode_reward)}\")\n",
        "                print (f\"mean cost: {np.mean(self.episode_cost)}\")\n",
        "                print (f\"c(x_0,a_0): {(self.ctable[0,states[0][0],states[0][1],a1])}\")\n",
        "\n",
        "            self.env.variation()\n",
        "        return self.episode_reward, self.episode_cost\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        succ = 0\n",
        "        test_cost = 0\n",
        "        for i in range(1):\n",
        "            self.env.reset_location()\n",
        "            actions_epi = []\n",
        "            locations = []    \n",
        "            finish = 0\n",
        "            locations.append(self.env.current_location)\n",
        "            for h in range(self.H):\n",
        "                old_state = self.env.current_location\n",
        "                action = self.use_action(h,old_state)\n",
        "                reward, cost, done = self.env.make_step(action) \n",
        "                succ += reward\n",
        "                test_cost += cost\n",
        "                actions_epi.append(action)\n",
        "                locations.append(self.env.current_location)\n",
        "        return succ, test_cost, finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRWcfm3x1Mbx"
      },
      "outputs": [],
      "source": [
        "cmdp_c = []\n",
        "cmdp_r = []\n",
        "cmdp_c_avg = []\n",
        "cmdp_r_cum = []\n",
        "episodics = 100000000\n",
        "for i in range(10):\n",
        "    np.random.seed(1234+1234*i)\n",
        "    env = GridWorld(K=1.75e6)\n",
        "    agent = Q_Agent_cmdp(env,show_z = True, use_ucb=True,use_lr=True, use_epsilon=False, decay_step=5,total_steps=episodics)\n",
        "    r,c = agent.play()\n",
        "    cmdp_r.append(r)\n",
        "    cmdp_c.append(c)\n",
        "    cmdp_r_cum.append(np.cumsum(r))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}